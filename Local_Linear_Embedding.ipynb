{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from numpy import matmul as mul\n",
    "import scipy as sp\n",
    "from sklearn import manifold\n",
    "from sklearn import neighbors\n",
    "from sklearn import datasets\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data from zipped mnist files into numpy arrays\n",
    "# Modified from source: https://gist.github.com/tylerneylon/ce60e8a06e7506ac45788443f7269e40\n",
    "#\n",
    "# Input:  filename --> Zipped file to parse\n",
    "#\n",
    "# Output: return   --> Numpy array of unint8 data points\n",
    "#\n",
    "def read_idx(filename):\n",
    "    with gzip.open(filename) as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an nxm matrix of evenly distributed samples from an input sample set\n",
    "#\n",
    "# Inputs: Y    --> Sample matrix\n",
    "#         n, m --> Landmark matrix dimensions\n",
    "#\n",
    "# Output: idx  --> nxm Landmark matrix\n",
    "#\n",
    "def find_landmarks(Y, n, m):\n",
    "    xr = np.linspace(np.min(Y[:,0]), np.max(Y[:,0]), n)\n",
    "    yr = np.linspace(np.min(Y[:,1]), np.max(Y[:,1]), m)\n",
    "    xg, yg = np.meshgrid(xr, yr)\n",
    "    idx = [0]*(n*m)\n",
    "    for i, x, y in zip(range(n*m), xg.flatten(), yg.flatten()):\n",
    "        idx[i] = int(np.sum(np.abs(Y-np.array([x,y]))**2, axis=-1).argmin())\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: x --> the basis vector (784 x 1)\n",
    "#         E --> eta is the matrix of nearest neighbors (k x 784)\n",
    "# \n",
    "# Outputs: C --> the covariance matrix of the nearest neighbors of the vector x \n",
    "#\n",
    "# Iterate theough the nearest neighbors and compute the  \n",
    "#\n",
    "def calc_covariance(Eta):\n",
    "    print(\"Calculate covariance:\")\n",
    "    print(\"E: \", np.shape(Eta))\n",
    "    \n",
    "    # Compute the local covariance matrix\n",
    "    C = np.cov(Eta)\n",
    "\n",
    "    print(\"Local covariance matrix C: \", np.shape(C))\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear non-neighbor weight entries from the weight vector\n",
    "#\n",
    "# Inputs:  nbrs --> the list of nearest neighbors\n",
    "#          wght --> the calculated weight vector\n",
    "# \n",
    "# Outputs: wght --> the adjusted weight vector    \n",
    "#\n",
    "def clear_non_neighbors(nbrs, wght):\n",
    "    print(\"Clear non-nieghbors:\")\n",
    "    print(\"wght: \", np.shape(wght), \" - \", wght)\n",
    "    \n",
    "    # Create mask vector\n",
    "    dim = np.shape(wght)[0]\n",
    "    mask = np.ones(dim)                        # ones mean, discard the value\n",
    "    print(\"Mask: \", np.shape(mask), \" - \", mask)\n",
    "\n",
    "    # Apply zeros to the nearest neighbors so we keep the weights\n",
    "    dim_nbrs = np.shape(nbrs)[0]\n",
    "    print(\"dim nbrs: \", dim_nbrs)\n",
    "    for i in range(0, dim_nbrs):\n",
    "        mask[nbrs[i]] = 0;\n",
    "        #print(\"nbr: \", nbrs[0,i])\n",
    "        \n",
    "    # mask off the non-neighbor weights\n",
    "    w = ma.masked_array(wght, mask)\n",
    "    print(w)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale weight values for nearest neighbors\n",
    "#\n",
    "# Inputs:  wght --> the wight vector with non-neighbors blanked out\n",
    "# \n",
    "# Outputs: w    --> the adjusted weight vector    \n",
    "#\n",
    "def scale_neighbors(wght):\n",
    "    sumw = np.sum(wght)\n",
    "    w = wght / sumw\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the reconstruction weights for each X\n",
    "#\n",
    "# Inputs:  X    --> Data Matrix\n",
    "#          v    --> Index into the data matrix for the vector we want to construct weights for\n",
    "#          tree --> nearest neighbors tree\n",
    "#          k    --> number of nearest neighbors  \n",
    "# \n",
    "# Outputs: W    --> the weight matrix\n",
    "#\n",
    "def construct_weight_vector(X, v, nbrs, k):\n",
    "    # Setup matrices and variables\n",
    "    D = np.shape(X)[1]                              # get the dimension of X\n",
    "    print(\"Construct weight matrix\")\n",
    "    print(\"X = \", np.shape(X))\n",
    "    print(\"D = \", D)\n",
    "    print(\"k = \", k)\n",
    "    print(\"Image #\", v, \"(v)\")\n",
    "    print(\"Neighbors = \", nbrs)\n",
    "\n",
    "    # Build a matrix of the nearest neighbors\n",
    "    # (this is all neighbors of the vector v, so remove all others from X)\n",
    "    Eta = np.delete(X, v, axis=0) \n",
    "    #print(\"Eta: \", np.shape(Eta))\n",
    "    \n",
    "    # Compute local covariance matrix for the vector v in X and all neighbors\n",
    "    C = calc_covariance(Eta)\n",
    "\n",
    "    # Solve the linear system with constraint that rows of weights sum to one\n",
    "    b = np.ones(np.shape(C)[1])  # build the constant vector\n",
    "    w = np.linalg.solve(C, b)    # compute solution\n",
    "    \n",
    "    # Apply first constraint to zero out non-neighbor weights\n",
    "    w = clear_non_neighbors(nbrs, w)\n",
    "    \n",
    "    # Apply second constraint to scale valid neighbors\n",
    "    w = scale_neighbors(w)\n",
    "    \n",
    "    print(\"w: \", np.shape(w))\n",
    "    #print(w)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find bottom d+1 eigenvectors of M\n",
    "#  (corresponding to the d+1 smallest eigenvalues) \n",
    "#set the qth ROW of Y to be the q+1 smallest eigenvector\n",
    "#  (discard the bottom eigenvector [1,1,1,1...] with eigenvalue zero)\n",
    "def compute_embedding_components(W, d):\n",
    "    print(\"Compute embedding components:\")\n",
    "    #create sparse matrix M = (I-W)'*(I-W)\n",
    "    I = np.identity(np.shape(W)[0])\n",
    "    M = mul((I - W).T, (I - W))\n",
    "\n",
    "    # Find the bottom d + 1 eigenvectors\n",
    "    U, S, Vt = np.linalg.svd(M)\n",
    "    \n",
    "    print(\"U: \", np.shape(U))\n",
    "    print(\"S: \", np.shape(S))\n",
    "    print(\"Vt: \", np.shape(Vt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the complete nearest neighbors array\n",
    "#\n",
    "# Inputs: X      --> training data\n",
    "#         n_nbrs --> the number of nearest neighbors to search for\n",
    "#         n_comp --> the number of components\n",
    "def compute_nearest_neighbors(X, n_nbrs, n_comp):\n",
    "    print(\"Computing nearest neighbors...\")\n",
    "    tree = neighbors.BallTree(X, leaf_size=n_comp)\n",
    "    dist, ind = tree.query(X, k=n_nbrs)\n",
    "    print(\"Complete nearest neighbor index: \", np.shape(ind))\n",
    "    return dist, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - We only have to implement this function, swap it with the manifold one below\n",
    "# Seek a low-rank projection on an input matrix\n",
    "#\n",
    "# Inputs: X            --> Input matrix to reduce\n",
    "#         n_neighbors  --> Maximum number of neighbors used for reconstruction\n",
    "#         n_components --> Maximum number of linearly independent components for reconstruction\n",
    "#\n",
    "# Output: Y            --> Reconstructed vectors in a lower rank\n",
    "#         err          --> (Optional implementation) Error margin of vectors\n",
    "#\n",
    "def locally_linear_embedding(X, n_neighbors, n_components):\n",
    "    \n",
    "    # Step 1: Find the nearest neighbors at for each sample\n",
    "    #tree = neighbors.BallTree(X, leaf_size=n_components)\n",
    "    #dist, ind = tree.query(X[:1], k=n_neighbors)\n",
    "    dist, ind = compute_nearest_neighbors(X, n_neighbors, n_components)\n",
    "    # ind = indices of k=n_neighbors nearest neighbors\n",
    "    # (nearest neighbors are ranked in order)\n",
    "    # dist = distance k=n_neighbors nearest neighbors\n",
    "    \n",
    "    # Step 2: Construct a weight matrix that recreates X from its neighbors\n",
    "    n = np.shape(X)[0]\n",
    "    d = np.shape(X)[1]\n",
    "    print(\"n: \", n)\n",
    "    print(\"d: \", d)\n",
    "    W = np.zeros([n,n-1])\n",
    "    print(\"Weight matrix: \", np.shape(W))\n",
    "\n",
    "    # Loop through each image and construct its weight matrix \n",
    "    rows = np.shape(X)[0]\n",
    "    for r in range(0, rows - 1):\n",
    "        print(\"Row: \",r)\n",
    "        print(\"ind: \", ind[r,:])\n",
    "        w = construct_weight_vector(X, r, ind[r,:], n_neighbors)\n",
    "        W[r,:] = w[:]\n",
    "    \n",
    "    # Step 3: Compute vectors that are reconstructed by weights\n",
    "    Y = compute_embedding_components(W, 2)\n",
    "    \n",
    "    Y = X[:,0:2]   # placeholder\n",
    "    err = 0.001    # placeholder\n",
    "    return Y, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mnist data from files\n",
    "raw_train = read_idx(\"train-images-idx3-ubyte.gz\")\n",
    "train_data = np.reshape(raw_train, (60000, 28*28))\n",
    "train_label = read_idx(\"train-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing nearest neighbors...\n"
     ]
    }
   ],
   "source": [
    "# Train algorithm and calculate landmark graph\n",
    "X = train_data[train_label == 8]\n",
    "#Y, err = manifold.locally_linear_embedding(X, n_neighbors=10, n_components=2)\n",
    "Y, err = locally_linear_embedding(X, n_neighbors=10, n_components=2)\n",
    "landmarks = find_landmarks(Y, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clustered data with landmarks overlaid\n",
    "plt.scatter(Y[:,0], Y[:,1])\n",
    "plt.scatter(Y[landmarks,0], Y[landmarks,1])\n",
    "\n",
    "# Show the landmark samples in a 5x5 grid\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "for i in range(len(landmarks)):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    imgplot = ax.imshow(np.reshape(X[landmarks[i]], (28,28)), cmap=plt.cm.get_cmap(\"Greys\"))\n",
    "    imgplot.set_interpolation(\"nearest\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
